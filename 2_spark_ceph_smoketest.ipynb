{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abroad-bishop",
   "metadata": {},
   "source": [
    "# Apache Spark is a must for big data processing\n",
    "Spark is likely going to be slower in Python than in Scala. PySpark allows you to write Spark applications using Python APIs and also provides the PySpark shell for interactively analyzing your data in a distributed environment. The ease of these notebooks and the popularity of Python make for a nice addition to the data science toolbox. [Find the docs here](https://spark.apache.org/docs/latest/api/python/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-adelaide",
   "metadata": {},
   "source": [
    "## Simple spark test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import os\n",
    "import socket\n",
    "\n",
    "# create a spark session\n",
    "spark_cluster_url = f\"spark://{os.environ['SPARK_CLUSTER']}:7077\"\n",
    "spark = SparkSession.builder.master(spark_cluster_url).getOrCreate()\n",
    "\n",
    "# test your spark connection\n",
    "spark.range(5, numPartitions=5).rdd.map(lambda x: socket.gethostname()).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-ridge",
   "metadata": {},
   "source": [
    "## This next part assumes you have Ceph or S3 Setup - fill in the bucket info for your storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit this section using your own credentials\n",
    "s3_region = 'region-1' # fill in for AWS, blank for Ceph\n",
    "s3_endpoint_url = 'https://s3.storage.server'\n",
    "s3_access_key_id = 'AccessKeyId-ChangeMe'\n",
    "s3_secret_access_key = 'SecretAccessKey-ChangeMe'\n",
    "s3_bucket = 'MyBucket'\n",
    "\n",
    "# for easy download\n",
    "!pip install wget\n",
    "\n",
    "import wget\n",
    "import boto3\n",
    "\n",
    "# configure boto S3 connection\n",
    "s3 = boto3.client('s3',\n",
    "                  s3_region,\n",
    "                  endpoint_url = s3_endpoint_url,\n",
    "                  aws_access_key_id = s3_access_key_id,\n",
    "                  aws_secret_access_key = s3_secret_access_key)\n",
    "\n",
    "# download the sample data file\n",
    "url = \"https://raw.githubusercontent.com/dudash/jupyter-gpu-examples/main/sample_data.csv\"\n",
    "file = wget.download(url=url, out='sample_data.csv')\n",
    "\n",
    "#upload the file to storage\n",
    "s3.upload_file(file, s3_bucket, \"sample_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-package",
   "metadata": {},
   "source": [
    "## Now let's test Spark with S3 and do basic Pandas data analysis call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "hadoopConf = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3a.endpoint\", s3_endpoint_url)\n",
    "hadoopConf.set(\"fs.s3a.access.key\", s3_access_key_id)\n",
    "hadoopConf.set(\"fs.s3a.secret.key\", s3_secret_access_key)\n",
    "hadoopConf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "hadoopConf.set(\"fs.s3a.connection.ssl.enabled\", \"true\") # false if not https\n",
    "\n",
    "data = spark.read.csv('s3a://' + s3_bucket + '/sample_data.csv',sep=\",\", header=True)\n",
    "df = data.toPandas()\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
